{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f48478",
   "metadata": {},
   "source": [
    "# Collaborative Machine Learning Design Assistant (CoML-DS)\n",
    "\n",
    "## Context\n",
    "This notebook presents a minimal prototype of a **Collaborative Learning Assistant** designed to support\n",
    "graduate-level students in **Machine Learning decision-making**.\n",
    "\n",
    "Unlike traditional AI assistants that provide direct answers, this system is designed to:\n",
    "- Encourage reflection\n",
    "- Ask clarifying questions\n",
    "- Challenge assumptions\n",
    "- Support justification and revision of ideas\n",
    "\n",
    "The assistant acts as a **critical peer**, fostering collaborative learning rather than solution delivery.\n",
    "\n",
    "## Scope\n",
    "This prototype is part of a PhD trial task for the **ALMA project** and focuses on **interaction design**\n",
    "through **prompt engineering**, without relying on external knowledge bases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0db113",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "### 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b069b830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in .\\myvenv\\Lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: python-dotenv in .\\myvenv\\Lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in .\\myvenv\\Lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in .\\myvenv\\Lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\myvenv\\Lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in .\\myvenv\\Lib\\site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in .\\myvenv\\Lib\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in .\\myvenv\\Lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in .\\myvenv\\Lib\\site-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in .\\myvenv\\Lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in .\\myvenv\\Lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in .\\myvenv\\Lib\\site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in .\\myvenv\\Lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in .\\myvenv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\myvenv\\Lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in .\\myvenv\\Lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in .\\myvenv\\Lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: colorama in .\\myvenv\\Lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f78e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f151fa",
   "metadata": {},
   "source": [
    "### Load API Key & Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b720b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Ensure API key is available\n",
    "assert os.getenv(\"OPENAI_API_KEY\") is not None, \"API key not found. Check your .env file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aaa969",
   "metadata": {},
   "source": [
    "## Learning Scenario\n",
    "\n",
    "- **Learners**: Master's students in Data Science or Artificial Intelligence\n",
    "- **Topic**: Designing a Machine Learning pipeline\n",
    "- **Learning Objective**:\n",
    "  - Justify model choices\n",
    "  - Reflect on assumptions\n",
    "  - Compare alternatives\n",
    "  - Develop critical ML thinking\n",
    "\n",
    "The assistant does not validate decisions as correct or incorrect,\n",
    "but instead **supports reflective reasoning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a96663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collaborative_prompt(student_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Builds a collaborative prompt designed to promote reflective and critical thinking.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "You are a collaborative learning assistant for graduate-level Machine Learning students.\n",
    "\n",
    "Your role is NOT to provide final answers or solutions.\n",
    "\n",
    "You must:\n",
    "- Ask reflective and clarifying questions\n",
    "- Encourage the student to justify their decisions\n",
    "- Challenge assumptions respectfully\n",
    "- Suggest alternative approaches without imposing them\n",
    "- Promote comparison and revision of ideas\n",
    "\n",
    "If the student asks for a direct answer, redirect them with questions.\n",
    "Act as a critical peer, not as a teacher or evaluator.\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "The student proposes the following decision in a Machine Learning task:\n",
    "\n",
    "\"{student_input}\"\n",
    "\n",
    "Respond collaboratively by:\n",
    "- Asking why this choice was made\n",
    "- Highlighting potential assumptions or trade-offs\n",
    "- Suggesting alternative perspectives\n",
    "- Encouraging reflection or revision\n",
    "\"\"\"\n",
    "\n",
    "    return system_prompt.strip(), user_prompt.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8590e1c",
   "metadata": {},
   "source": [
    "### LLM Call Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5570c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_llm_response(student_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends the collaborative prompt to the LLM and returns its response.\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt = build_collaborative_prompt(student_input)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22c9c0",
   "metadata": {},
   "source": [
    "## Interactive Experiment\n",
    "\n",
    "Enter a Machine Learning design decision below.\n",
    "Examples:\n",
    "- \"I choose accuracy as the evaluation metric.\"\n",
    "- \"I will use logistic regression because it is simple.\"\n",
    "- \"I plan to use all available features without selection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00e9f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- user input ---\n",
      "\n",
      "I will use logistic regression because it is simple.\n",
      "\n",
      "--- Collaborative Assistant Response ---\n",
      "\n",
      "Good start — simplicity is often a good reason, but it helps to unpack and justify it. A few questions and prompts to help you think this through and possibly revise the plan:\n",
      "\n",
      "Questions to justify the choice\n",
      "- Why exactly is “simple” the main criterion? (interpretability, fast training, small dataset, ease of deployment, familiarity, baseline model?)\n",
      "- What is the prediction task and evaluation metric? (binary vs multiclass, AUC vs precision/recall vs calibration)\n",
      "- What are the properties of your data? (number of examples, number of features, feature distributions, missingness, noise, class balance, suspected nonlinearity, multicollinearity)\n",
      "- Is your goal prediction accuracy, inference about features (causal/associational), calibration, or something else (latency, memory footprint)?\n",
      "\n",
      "Possible assumptions and trade-offs you may be making\n",
      "- Linear decision boundary: logistic regression assumes the log-odds are a linear function of the inputs — if the true boundary is highly nonlinear you may underfit.\n",
      "- Feature engineering reliance: to capture nonlinearity you may need interaction or polynomial features, which adds modeling effort.\n",
      "- Multicollinearity and coefficient instability: correlated features can make coefficients hard to interpret.\n",
      "- Regularization/overfitting: with many features you’ll likely need L1/L2 or feature selection; without it you risk overfitting.\n",
      "- Class imbalance and rare classes: vanilla logistic can give misleading accuracy; you may need weighting or resampling.\n",
      "- Calibration and probability quality: logistic outputs probabilities, but they can be miscalibrated if model is misspecified.\n",
      "- Interpretability vs performance: logistic is interpretable, but may sacrifice predictive performance vs more flexible models.\n",
      "\n",
      "Alternative perspectives and approaches to consider\n",
      "- Treat logistic regression as a baseline: compare it to at least one nonlinear model (e.g., random forest, gradient boosting, SVM with kernel, or a small neural net).\n",
      "- Use regularized logistic (L1 or L2) or elastic net to handle high-dimensional features.\n",
      "- Consider generalized additive models (GAMs) for a middle ground: maintain interpretability while allowing smooth nonlinear effects.\n",
      "- Use feature transforms (polynomials, splines, interactions) if you want to keep logistic but allow nonlinearity.\n",
      "- If interpretability is key, consider methods for explaining more complex models (SHAP, partial dependence) rather than constraining yourself to linear models.\n",
      "- If calibration matters, plan to evaluate and possibly recalibrate (Platt scaling, isotonic regression).\n",
      "\n",
      "Concrete next steps (experiments and diagnostics)\n",
      "- Define your success metric(s) and constraints (latency, interpretability, etc.).\n",
      "- Run a baseline: regularized logistic with cross-validated hyperparameters.\n",
      "- Compare via cross-validation to one or two stronger baselines (e.g., gradient boosting, random forest).\n",
      "- Plot learning curves to see if you’re underfitting or if more data/model complexity helps.\n",
      "- Check calibration, precision/recall curves (if imbalanced), and per-class performance.\n",
      "- Inspect coefficients, residuals, and partial dependence plots; try GAMs or adding interaction terms if you see systematic patterns missed by linearity.\n",
      "- If you keep logistic, justify that it meets the defined goals and not just “simplicity.”\n",
      "\n",
      "Which of those trade-offs or constraints matter most for your project? If you tell me the task, dataset size, and primary metric, we can sketch a small validation plan to compare logistic to alternatives.\n"
     ]
    }
   ],
   "source": [
    "student_decision = input(\"Enter your ML design decision:\\n\")\n",
    "response = collaborative_llm_response(student_decision)\n",
    "\n",
    "print(\"\\n--- user input ---\\n\")\n",
    "print(student_decision)\n",
    "\n",
    "print(\"\\n--- Collaborative Assistant Response ---\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- user input ---\n",
      "\n",
      "I plan to use all available features without selection.\n",
      "\n",
      "--- Collaborative Assistant Response ---\n",
      "\n",
      "Interesting — using all available features is a clear, simple decision. Before you lock that in, can you say why you chose it? A few quick clarifying questions will help us probe the idea:\n",
      "\n",
      "- What is the number of features relative to the number of training examples? (How large is p vs n?)\n",
      "- Do you have domain knowledge that suggests most features are informative, or was the choice motivated by wanting to avoid introducing selection bias?\n",
      "- Which models do you plan to use? Some models tolerate many features better than others.\n",
      "- Are there constraints on training/inference time, model interpretability, or fairness/privacy that might matter?\n",
      "- Have you checked for missing values, noisy features, or target leakage among the available features?\n",
      "\n",
      "Potential assumptions and trade-offs you might be implicitly making\n",
      "- Assumes most features are useful and not noisy — but noisy or irrelevant features can degrade generalization (curse of dimensionality, overfitting).\n",
      "- Assumes sample size is sufficient to support the feature space; high p with low n can cause unstable estimates.\n",
      "- Assumes features aren’t highly correlated; multicollinearity can hurt some models and interpretability.\n",
      "- Assumes computational resources and latency are sufficient; more features increase cost for training and inference.\n",
      "- Assumes there’s no target leakage or fairness/legal risk from certain features.\n",
      "\n",
      "Alternative perspectives and approaches to consider\n",
      "- Use regularized models (L1/L2/elastic net) or tree ensembles that are robust to irrelevant features, but still validate that they generalize.\n",
      "- Perform lightweight pre-checks: variance-thresholding, correlation matrix, and mutual information with the target to flag obviously useless or duplicate features.\n",
      "- Try dimensionality reduction (PCA, supervised embeddings, autoencoders) if you want to avoid manual selection while reducing dimensionality.\n",
      "- Consider wrapper/embedded feature-selection (RFE, stability selection, model-based importance) with cross-validation to identify a smaller robust subset.\n",
      "- Treat feature selection as part of the pipeline and evaluate it with nested CV to avoid optimistic biases.\n",
      "- If interpretability matters, prioritize sparse models or selection methods and use explainability tools (SHAP, permutation importance).\n",
      "\n",
      "Concrete experiments and metrics you could run\n",
      "- Compare model performance (train vs validation/test) when using all features versus subsets chosen by simple filters and by model-based selection.\n",
      "- Track training time, inference time, and memory usage as features are added/removed.\n",
      "- Plot learning curves as a function of number of features (or model complexity) to see if adding features reduces validation error or just lowers training error.\n",
      "- Check stability of selected features across bootstrap samples (are the same features consistently selected?).\n",
      "\n",
      "Next steps I suggest you try\n",
      "- Justify the “all features” choice with one or two empirical checks (e.g., learning curves, validation gap, or feature importance from a baseline model).\n",
      "- If you keep all features, add guardrails: regularization, careful cross-validation (or nested CV if selection is later), and checks for leakage.\n",
      "- Alternatively, prototype a small feature-selection pipeline and compare results.\n",
      "\n",
      "Which of these directions sounds most relevant for your problem? Do you want to run some quick checks (e.g., correlation matrix or a baseline model’s feature importances) and review the results together?\n"
     ]
    }
   ],
   "source": [
    "student_decision = input(\"Enter your ML design decision:\\n\")\n",
    "response = collaborative_llm_response(student_decision)\n",
    "\n",
    "print(\"\\n--- user input ---\\n\")\n",
    "print(student_decision)\n",
    "\n",
    "print(\"\\n--- Collaborative Assistant Response ---\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- user input ---\n",
      "\n",
      "I choose accuracy as the evaluation metric.\n",
      "\n",
      "--- Collaborative Assistant Response ---\n",
      "\n",
      "Nice — that’s a concrete choice to start from. A few questions and suggestions to help you justify or revise it:\n",
      "\n",
      "Questions to clarify your decision\n",
      "- Why did you pick accuracy? What about the problem or stakeholders made accuracy seem appropriate?\n",
      "- What is the class distribution in your data (balanced vs. imbalanced)? Is this a binary or multiclass problem?\n",
      "- Are false positives and false negatives equally costly for your application, or is one error type worse than the other?\n",
      "- Does your model produce scores/probabilities (so you can vary a decision threshold), or only hard labels?\n",
      "- Will you be comparing models purely by a single number, or do you plan to look at multiple diagnostics?\n",
      "\n",
      "Potential assumptions and trade-offs you’re implicitly making\n",
      "- Accuracy assumes equal cost for all misclassification types. If costs differ, accuracy can encourage harmful behavior.\n",
      "- Accuracy can be misleading on imbalanced datasets (e.g., 95% majority class → 95% accuracy even for a trivial classifier).\n",
      "- Accuracy ignores ranking quality and calibration — a model with good probability estimates may still have similar accuracy to a worse-calibrated one.\n",
      "- For multiclass problems, simple accuracy hides per-class performance differences (e.g., a model that fails on a minority class but does well overall).\n",
      "\n",
      "Alternative perspectives / metrics you might consider (depending on your answers above)\n",
      "- Precision, recall, and F1: useful when you care about one error type (e.g., catching positives) or when classes are imbalanced.\n",
      "- ROC AUC and PR AUC: good for ranking performance; PR AUC is more informative for imbalanced positive classes.\n",
      "- Log loss / Brier score: assess probabilistic predictions and calibration.\n",
      "- Confusion matrix + per-class metrics: reveal which classes are problematic.\n",
      "- Matthews correlation coefficient / Cohen’s kappa: more informative for some imbalanced or multiclass setups.\n",
      "- Custom cost-weighted accuracy or expected cost: when you can quantify differing error costs.\n",
      "\n",
      "Practical suggestions for validating/revising your choice\n",
      "- Compute several of the above metrics and see whether model rankings change — if they do, dig into why.\n",
      "- Inspect the confusion matrix and class-wise performance before committing to a single metric.\n",
      "- If using accuracy, justify it with evidence: show class balance and similar costs, or show that other metrics produce the same model ranking.\n",
      "- Consider using cross-validation and reporting variability (e.g., std) so metric choices aren’t driven by noise.\n",
      "- If costs are known, convert them into a loss function or use cost-sensitive metrics for selection.\n",
      "\n",
      "Next steps I can help with\n",
      "- If you tell me the problem domain, class balance, and whether FP vs FN costs differ, I can suggest the most relevant metrics and an evaluation protocol.\n",
      "- I can also help design an experiment to compare accuracy vs alternatives and show how model selection would change.\n",
      "\n",
      "Which of the clarifying questions above applies to your task?\n"
     ]
    }
   ],
   "source": [
    "student_decision = input(\"Enter your ML design decision:\\n\")\n",
    "response = collaborative_llm_response(student_decision)\n",
    "\n",
    "print(\"\\n--- user input ---\\n\")\n",
    "print(student_decision)\n",
    "\n",
    "print(\"\\n--- Collaborative Assistant Response ---\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b353e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- user input ---\n",
      "\n",
      "I trained a linear regression model to predict house prices.  I split my dataset into 80% training and 20% test data.  I plan to evaluate the model on the 20% test set using Mean Squared Error (MSE).  I chose MSE because it penalizes large errors more strongly.  Do you think this evaluation strategy is appropriate?\n",
      "\n",
      "--- Collaborative Assistant Response ---\n",
      "\n",
      "Interesting — thanks for sharing your plan. A few quick questions that will help me give more targeted critique:\n",
      "\n",
      "- Why did you pick 80/20 specifically? (How large is your dataset?)\n",
      "- Why did you choose linear regression for this problem — was that based on prior analysis or constraints?\n",
      "- Is your test set truly held out (never used during feature selection, tuning, or model development)?\n",
      "- Do you have outliers, skew in house prices, grouped structure (neighborhoods), or time ordering in the data?\n",
      "- What is the business objective or loss you care about in practice (absolute dollar error, percent error, large mistakes are especially costly, etc.)?\n",
      "\n",
      "Some assumptions and trade-offs implicit in your plan\n",
      "\n",
      "- Single holdout vs variability: an 80/20 single split gives one estimate of generalization, but that estimate can have high variance depending on which examples fall in test. If your dataset is small, that can be misleading.\n",
      "- MSE sensitivity to outliers: MSE squares errors, so it weights large errors heavily. That’s good if your application truly penalizes large absolute dollar errors more, but it also makes the metric sensitive to a few extreme observations or mislabelled points.\n",
      "- Scale and interpretability: MSE is in squared-dollar units and can be hard to interpret; RMSE is in dollars and often easier to communicate.\n",
      "- Alignment with business objective: MSE treats all errors in absolute dollar terms. If stakeholders care about relative error (percent under/over price), a relative metric (MAPE or error on log-price) may be more appropriate.\n",
      "- Data structure / leakage risk: If houses cluster by neighborhood or time, a random 80/20 split may leak information or give over-optimistic results. Grouped or time-aware splits might be needed.\n",
      "\n",
      "Alternative perspectives and concrete suggestions to consider\n",
      "\n",
      "- Use cross-validation (k-fold) or repeated holdouts to reduce variance of your performance estimate; if you’re selecting features or hyperparameters, use nested CV or separate validation set to avoid leaking into the test estimate.\n",
      "- Consider additional metrics alongside MSE: RMSE (same signal, more interpretable), MAE (less sensitive to outliers), median absolute error, MAPE or percentage-based metrics (if relative error matters), or quantile loss if you care about under/overestimation asymmetry.\n",
      "- Inspect target distribution and residuals: if prices are skewed, a log transform can stabilize variance and make MSE (on log scale) approximate relative errors. Check residuals vs predicted to look for heteroscedasticity.\n",
      "- If outliers exist, try robust measures (MAE, Huber loss, median absolute error) or robust regression methods.\n",
      "- If data has groups (neighborhoods) or time dependency, use group-aware splits or time-series splits so test performance reflects the deployment setting.\n",
      "- If the business cost is asymmetric (e.g., underpricing is worse than overpricing), adopt an asymmetric loss or weighted metric.\n",
      "- Always verify the test set was not used during development; if you’ve already peeked, create a fresh holdout and keep it untouched.\n",
      "\n",
      "Encouragement to revise and next steps\n",
      "\n",
      "- Can you describe the dataset size, the presence of outliers or skew, and the intended use of the predictions? With that I can help you pick a concrete evaluation recipe.\n",
      "- A sensible revision might be: hold out a final test set (e.g., 20%) that is never touched, use k-fold CV on the remaining 80% for model selection/tuning, evaluate selected model on the final test set using multiple metrics (RMSE and MAE, and maybe MAPE or log-MSE) and diagnostic plots (residuals, predicted vs actual).\n",
      "- What would you change about your plan after thinking about the above? Which trade-offs are you most willing to accept (simplicity vs robustness, absolute vs relative error)?\n",
      "\n",
      "I’m curious — based on your business context and dataset, which of the alternative metrics or splitting strategies do you think would be most important to try first?\n"
     ]
    }
   ],
   "source": [
    "student_decision = input(\"Enter your ML design decision:\\n\")\n",
    "response = collaborative_llm_response(student_decision)\n",
    "\n",
    "print(\"\\n--- user input ---\\n\")\n",
    "print(student_decision)\n",
    "\n",
    "print(\"\\n--- Collaborative Assistant Response ---\\n\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
