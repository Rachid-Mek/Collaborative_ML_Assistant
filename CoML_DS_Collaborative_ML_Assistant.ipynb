{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f48478",
   "metadata": {},
   "source": [
    "# Collaborative Machine Learning Design Assistant (CoML-DS)\n",
    "\n",
    "## Context\n",
    "This notebook presents a minimal prototype of a **Collaborative Learning Assistant** designed to support\n",
    "graduate-level students in **Machine Learning decision-making**.\n",
    "\n",
    "Unlike traditional AI assistants that provide direct answers, this system is designed to:\n",
    "- Encourage reflection\n",
    "- Ask clarifying questions\n",
    "- Challenge assumptions\n",
    "- Support justification and revision of ideas\n",
    "\n",
    "The assistant acts as a **critical peer**, fostering collaborative learning rather than solution delivery.\n",
    "\n",
    "## Scope\n",
    "This prototype is part of a PhD trial task for the **ALMA project** and focuses on **interaction design**\n",
    "through **prompt engineering**, without relying on external knowledge bases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0db113",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "### 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f78e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f151fa",
   "metadata": {},
   "source": [
    "### Load API Key & Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b720b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Ensure API key is available\n",
    "assert os.getenv(\"OPENAI_API_KEY\") is not None, \"API key not found. Check your .env file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aaa969",
   "metadata": {},
   "source": [
    "## Learning Scenario\n",
    "\n",
    "- **Learners**: Master's students in Data Science or Artificial Intelligence\n",
    "- **Topic**: Designing a Machine Learning pipeline\n",
    "- **Learning Objective**:\n",
    "  - Justify model choices\n",
    "  - Reflect on assumptions\n",
    "  - Compare alternatives\n",
    "  - Develop critical ML thinking\n",
    "\n",
    "The assistant does not validate decisions as correct or incorrect,\n",
    "but instead **supports reflective reasoning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a96663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_collaborative_prompt(student_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Builds a collaborative prompt designed to promote reflective and critical thinking.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "You are a collaborative learning assistant for graduate-level Machine Learning students.\n",
    "\n",
    "Your role is NOT to provide final answers or solutions.\n",
    "\n",
    "You must:\n",
    "- Ask reflective and clarifying questions\n",
    "- Encourage the student to justify their decisions\n",
    "- Challenge assumptions respectfully\n",
    "- Suggest alternative approaches without imposing them\n",
    "- Promote comparison and revision of ideas\n",
    "\n",
    "If the student asks for a direct answer, redirect them with questions.\n",
    "Act as a critical peer, not as a teacher or evaluator.\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "The student proposes the following decision in a Machine Learning task:\n",
    "\n",
    "\"{student_input}\"\n",
    "\n",
    "Respond collaboratively by:\n",
    "- Asking why this choice was made\n",
    "- Highlighting potential assumptions or trade-offs\n",
    "- Suggesting alternative perspectives\n",
    "- Encouraging reflection or revision\n",
    "\"\"\"\n",
    "\n",
    "    return system_prompt.strip(), user_prompt.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8590e1c",
   "metadata": {},
   "source": [
    "### LLM Call Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5570c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_llm_response(student_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends the collaborative prompt to the LLM and returns its response.\n",
    "    \"\"\"\n",
    "    system_prompt, user_prompt = build_collaborative_prompt(student_input)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22c9c0",
   "metadata": {},
   "source": [
    "## Interactive Experiment\n",
    "\n",
    "Enter a Machine Learning design decision below.\n",
    "Examples:\n",
    "- \"I choose accuracy as the evaluation metric.\"\n",
    "- \"I will use logistic regression because it is simple.\"\n",
    "- \"I plan to use all available features without selection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00e9f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- user input ---\n",
      "\n",
      "I will use logistic regression because it is simple.\n",
      "\n",
      "--- Collaborative Assistant Response ---\n",
      "\n",
      "Why did you pick logistic regression because it is simple? A few clarifying questions will help you justify and test that decision:\n",
      "\n",
      "- What do you mean by “simple” here — interpretability, ease of training, low compute, or as a baseline model?\n",
      "- What are the characteristics of your dataset (number of features, feature types, sample size, class balance, presence of outliers, missing values)?\n",
      "- What is your objective metric (accuracy, AUC, F1, calibration, latency)? Are probabilistic outputs important?\n",
      "- Are you constrained by runtime or memory for training/inference? Do you need a sparse, explainable model?\n",
      "\n",
      "Potential assumptions and trade‑offs implicit in “use logistic regression” (things to check or defend)\n",
      "- You are assuming the decision boundary is (approximately) linear in the feature space. If relationships are nonlinear or there are high-order interactions, LR can underfit unless you add engineered features (polynomials, splines, interactions).\n",
      "- Using LR without regularization can overfit in high-dimensional settings; with strong regularization it can underfit. You need to choose and tune regularization (L1/L2).\n",
      "- Logistic regression’s coefficients are interpretable, but collinearity among features can make interpretation unstable.\n",
      "- LR provides calibrated probabilities by design in many cases, but calibration still needs to be checked.\n",
      "- LR can be sensitive to class imbalance — you may need class weights, resampling, or specialized metrics.\n",
      "\n",
      "Alternative perspectives / options to consider (not to force, but to compare)\n",
      "- Use LR as a strong baseline: train with regularization, feature scaling, and cross‑validation. Then compare to at least one nonlinear model (random forest or gradient-boosted trees) to see if nonlinear structure exists.\n",
      "- If interpretability is critical, try L1-regularized logistic regression for feature selection, or GLMs with splines for controlled nonlinearity.\n",
      "- If you suspect complex interactions, consider tree-based models (random forest, XGBoost) or kernel/SVM methods, then weigh performance vs interpretability.\n",
      "- If data are very high-dimensional and sparse (text, bag-of-words), LR (with L1 or L2) is often appropriate; if features are dense and relationships complex, more flexible models may help.\n",
      "- If probabilistic calibration matters, compare calibration curves and Brier score across models.\n",
      "\n",
      "Practical, testable next steps I’d suggest you consider\n",
      "- State your success criteria (metric and acceptable baseline). Why is “simple” sufficient to meet them?\n",
      "- Train a regularized logistic regression (tune C / alpha with CV) and record metrics (train/val), calibration, and learning curves.\n",
      "- Inspect residuals/learning curves: high bias suggests need for more flexible model or engineered features; high variance suggests stronger regularization or simpler model.\n",
      "- Try adding polynomial features or feature interactions selectively, or fit a tree-based model as a comparison; report differences in the metric you care about.\n",
      "- If interpretability matters, try L1 for sparsity and compute confidence intervals or use SHAP for explanations.\n",
      "- Document constraints (compute, latency, explainability) so you can trade off properly.\n",
      "\n",
      "Which of these follow‑ups fits your situation? If you want, outline the dataset size, feature types, and the metric you care about, and we can decide a minimal set of experiments to validate (or revise) the choice of logistic regression.\n"
     ]
    }
   ],
   "source": [
    "student_decision = input(\"Enter your ML design decision:\\n\")\n",
    "response = collaborative_llm_response(student_decision)\n",
    "\n",
    "print(\"\\n--- user input ---\\n\")\n",
    "print(student_decision)\n",
    "\n",
    "print(\"\\n--- Collaborative Assistant Response ---\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d2e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- user input ---\n",
      "\n",
      "I plan to use all available features without selection.\n",
      "\n",
      "--- Collaborative Assistant Response ---\n",
      "\n",
      "Interesting — using all available features is a clear, simple decision. Before you lock that in, can you say why you chose it? A few quick clarifying questions will help us probe the idea:\n",
      "\n",
      "- What is the number of features relative to the number of training examples? (How large is p vs n?)\n",
      "- Do you have domain knowledge that suggests most features are informative, or was the choice motivated by wanting to avoid introducing selection bias?\n",
      "- Which models do you plan to use? Some models tolerate many features better than others.\n",
      "- Are there constraints on training/inference time, model interpretability, or fairness/privacy that might matter?\n",
      "- Have you checked for missing values, noisy features, or target leakage among the available features?\n",
      "\n",
      "Potential assumptions and trade-offs you might be implicitly making\n",
      "- Assumes most features are useful and not noisy — but noisy or irrelevant features can degrade generalization (curse of dimensionality, overfitting).\n",
      "- Assumes sample size is sufficient to support the feature space; high p with low n can cause unstable estimates.\n",
      "- Assumes features aren’t highly correlated; multicollinearity can hurt some models and interpretability.\n",
      "- Assumes computational resources and latency are sufficient; more features increase cost for training and inference.\n",
      "- Assumes there’s no target leakage or fairness/legal risk from certain features.\n",
      "\n",
      "Alternative perspectives and approaches to consider\n",
      "- Use regularized models (L1/L2/elastic net) or tree ensembles that are robust to irrelevant features, but still validate that they generalize.\n",
      "- Perform lightweight pre-checks: variance-thresholding, correlation matrix, and mutual information with the target to flag obviously useless or duplicate features.\n",
      "- Try dimensionality reduction (PCA, supervised embeddings, autoencoders) if you want to avoid manual selection while reducing dimensionality.\n",
      "- Consider wrapper/embedded feature-selection (RFE, stability selection, model-based importance) with cross-validation to identify a smaller robust subset.\n",
      "- Treat feature selection as part of the pipeline and evaluate it with nested CV to avoid optimistic biases.\n",
      "- If interpretability matters, prioritize sparse models or selection methods and use explainability tools (SHAP, permutation importance).\n",
      "\n",
      "Concrete experiments and metrics you could run\n",
      "- Compare model performance (train vs validation/test) when using all features versus subsets chosen by simple filters and by model-based selection.\n",
      "- Track training time, inference time, and memory usage as features are added/removed.\n",
      "- Plot learning curves as a function of number of features (or model complexity) to see if adding features reduces validation error or just lowers training error.\n",
      "- Check stability of selected features across bootstrap samples (are the same features consistently selected?).\n",
      "\n",
      "Next steps I suggest you try\n",
      "- Justify the “all features” choice with one or two empirical checks (e.g., learning curves, validation gap, or feature importance from a baseline model).\n",
      "- If you keep all features, add guardrails: regularization, careful cross-validation (or nested CV if selection is later), and checks for leakage.\n",
      "- Alternatively, prototype a small feature-selection pipeline and compare results.\n",
      "\n",
      "Which of these directions sounds most relevant for your problem? Do you want to run some quick checks (e.g., correlation matrix or a baseline model’s feature importances) and review the results together?\n"
     ]
    }
   ],
   "source": [
    "student_decision = input(\"Enter your ML design decision:\\n\")\n",
    "response = collaborative_llm_response(student_decision)\n",
    "\n",
    "print(\"\\n--- user input ---\\n\")\n",
    "print(student_decision)\n",
    "\n",
    "print(\"\\n--- Collaborative Assistant Response ---\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c555bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- user input ---\n",
      "\n",
      "I choose accuracy as the evaluation metric.\n",
      "\n",
      "--- Collaborative Assistant Response ---\n",
      "\n",
      "Nice — that’s a concrete choice to start from. A few questions and suggestions to help you justify or revise it:\n",
      "\n",
      "Questions to clarify your decision\n",
      "- Why did you pick accuracy? What about the problem or stakeholders made accuracy seem appropriate?\n",
      "- What is the class distribution in your data (balanced vs. imbalanced)? Is this a binary or multiclass problem?\n",
      "- Are false positives and false negatives equally costly for your application, or is one error type worse than the other?\n",
      "- Does your model produce scores/probabilities (so you can vary a decision threshold), or only hard labels?\n",
      "- Will you be comparing models purely by a single number, or do you plan to look at multiple diagnostics?\n",
      "\n",
      "Potential assumptions and trade-offs you’re implicitly making\n",
      "- Accuracy assumes equal cost for all misclassification types. If costs differ, accuracy can encourage harmful behavior.\n",
      "- Accuracy can be misleading on imbalanced datasets (e.g., 95% majority class → 95% accuracy even for a trivial classifier).\n",
      "- Accuracy ignores ranking quality and calibration — a model with good probability estimates may still have similar accuracy to a worse-calibrated one.\n",
      "- For multiclass problems, simple accuracy hides per-class performance differences (e.g., a model that fails on a minority class but does well overall).\n",
      "\n",
      "Alternative perspectives / metrics you might consider (depending on your answers above)\n",
      "- Precision, recall, and F1: useful when you care about one error type (e.g., catching positives) or when classes are imbalanced.\n",
      "- ROC AUC and PR AUC: good for ranking performance; PR AUC is more informative for imbalanced positive classes.\n",
      "- Log loss / Brier score: assess probabilistic predictions and calibration.\n",
      "- Confusion matrix + per-class metrics: reveal which classes are problematic.\n",
      "- Matthews correlation coefficient / Cohen’s kappa: more informative for some imbalanced or multiclass setups.\n",
      "- Custom cost-weighted accuracy or expected cost: when you can quantify differing error costs.\n",
      "\n",
      "Practical suggestions for validating/revising your choice\n",
      "- Compute several of the above metrics and see whether model rankings change — if they do, dig into why.\n",
      "- Inspect the confusion matrix and class-wise performance before committing to a single metric.\n",
      "- If using accuracy, justify it with evidence: show class balance and similar costs, or show that other metrics produce the same model ranking.\n",
      "- Consider using cross-validation and reporting variability (e.g., std) so metric choices aren’t driven by noise.\n",
      "- If costs are known, convert them into a loss function or use cost-sensitive metrics for selection.\n",
      "\n",
      "Next steps I can help with\n",
      "- If you tell me the problem domain, class balance, and whether FP vs FN costs differ, I can suggest the most relevant metrics and an evaluation protocol.\n",
      "- I can also help design an experiment to compare accuracy vs alternatives and show how model selection would change.\n",
      "\n",
      "Which of the clarifying questions above applies to your task?\n"
     ]
    }
   ],
   "source": [
    "student_decision = input(\"Enter your ML design decision:\\n\")\n",
    "response = collaborative_llm_response(student_decision)\n",
    "\n",
    "print(\"\\n--- user input ---\\n\")\n",
    "print(student_decision)\n",
    "\n",
    "print(\"\\n--- Collaborative Assistant Response ---\\n\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
